{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a28911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7343641",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Data from Multiple Sources\n",
    "\n",
    "Pandas provides **flexible data ingestion** capabilities that allow you to read data from various sources. The `read_csv()` function is particularly versatile, accepting both **local file paths** and **public URLs**. This makes it easy to work with data stored on your computer or hosted online without changing your code structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378c85c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read from a URL\n",
    "url = \"https://raw.githubusercontent.com/alanjones2/uk-historical-weather/refs/heads/main/data/Cardiff_Bute_Park.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Or read from a local file\n",
    "# df = pd.read_csv('data/weather.csv')\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1c9a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "Before manipulating data, always **understand its structure**. Check the **shape**, **column names**, **data types**, and **missing values**. The `.info()` method provides a comprehensive overview in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d3b44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Comprehensive info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbe030",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Summaries with `.describe()`\n",
    "\n",
    "The `.describe()` method computes **summary statistics** (count, mean, std, min, max, quartiles) for all numerical columns. The output is itself a DataFrame, allowing further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f3e68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "stats = df.describe()\n",
    "print(stats)\n",
    "\n",
    "# Access specific statistics\n",
    "print(f\"\\nMean temperature: {stats.loc['mean', 'Tmean']:.2f}\u00b0C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c61e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converting to Datetime Types\n",
    "\n",
    "Date columns are often read as **text** by default. Converting them to **datetime objects** using `pd.to_datetime()` unlocks temporal operations like filtering by date ranges, extracting components, and creating time-aware visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6d091",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert text to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(f\"Date type: {df['Date'].dtype}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "# Now we can filter by dates\n",
    "recent_data = df[df['Date'] > '2020-01-01']\n",
    "print(f\"\\nRows after 2020: {len(recent_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3429f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting a Meaningful Index\n",
    "\n",
    "For time series data, using the **date as the index** simplifies plotting and slicing. Use `set_index()` to assign a column as the index and `.drop(axis=1)` to remove unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e167b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set date as index\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print(\"New structure:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\nIndex name: {df.index.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c308eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detecting Missing Values\n",
    "\n",
    "Real-world data often has **missing values** (`NaN`). Use `.isnull()` to identify them and `.sum()` to count missing values per column. Understanding **where and why** data is missing guides your handling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82d8bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing)\n",
    "\n",
    "# Find rows with missing values in a specific column\n",
    "missing_sun = df[df['Sun'].isnull()]\n",
    "print(f\"\\nRows with missing Sun data: {len(missing_sun)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d1ef4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strategies for Handling Missing Data\n",
    "\n",
    "Three main approaches: **drop** rows/columns (sparse data), **fill** with values like mean or median (simple imputation), or **interpolate** from surrounding values (time series). Choose based on your **data's nature** and **missingness patterns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecdef5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Strategy 1: Drop columns with mostly missing data\n",
    "df = df.drop('status', axis=1)\n",
    "\n",
    "# Strategy 2: Fill with a value\n",
    "df['Rain'] = df['Rain'].fillna(0)\n",
    "\n",
    "# Strategy 3: Interpolate\n",
    "df['Sun'] = df['Sun'].interpolate(method='linear')\n",
    "\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa32df9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation for Time Series\n",
    "\n",
    "**Interpolation** estimates missing values from neighboring points. **Linear interpolation** draws straight lines between known values. More sophisticated methods (polynomial, spline) exist but require more computation and careful application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85869a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create sample data with gaps\n",
    "data = pd.Series([1.0, 2.0, np.nan, np.nan, 5.0, 6.0])\n",
    "print(\"Before interpolation:\")\n",
    "print(data)\n",
    "\n",
    "# Linear interpolation\n",
    "data_interp = data.interpolate(method='linear')\n",
    "print(\"\\nAfter interpolation:\")\n",
    "print(data_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b1d37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting Date Components\n",
    "\n",
    "Datetime objects have useful **attributes** like `.year`, `.month`, `.day`, `.dayofweek`, etc. Extract **month names** with `.month_name()` or quarters with `.quarter` for seasonal analysis and grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee06fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Extract date components from the index\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "df['Month_Name'] = df.index.month_name()\n",
    "df['Quarter'] = df.index.quarter\n",
    "\n",
    "print(df[['Year', 'Month', 'Month_Name', 'Quarter']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0d02e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functional Programming with `.apply()`\n",
    "\n",
    "Use `.apply()` to apply **custom functions** to DataFrame columns. Functions are **first-class objects** that can be passed as arguments, enabling you to create derived features with custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceec02b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a custom function\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "# Apply it to create a new column\n",
    "df['Season'] = df['Month'].apply(get_season)\n",
    "print(df[['Month', 'Month_Name', 'Season']].head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044cd5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Derived Features\n",
    "\n",
    "Create new columns through **arithmetic operations** or **boolean conditions**. Derived features encode domain knowledge into your data structure, like temperature ranges or threshold-based flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e535b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Arithmetic operation\n",
    "df['Temp_Range'] = df['Tmax'] - df['Tmin']\n",
    "\n",
    "# Boolean conditions\n",
    "df['Rainy_Month'] = df['Rain'] > 100\n",
    "df['Sunny_Month'] = df['Sun'] > 150\n",
    "\n",
    "print(df[['Tmax', 'Tmin', 'Temp_Range', 'Rain', 'Rainy_Month']].head())\n",
    "print(f\"\\nRainy months: {df['Rainy_Month'].sum()} ({df['Rainy_Month'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02994d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grouping and Aggregation\n",
    "\n",
    "The `.groupby()` method **aggregates data** by categories. Chain three operations: group by a column, select columns to aggregate, and apply an aggregation function (mean, sum, count, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f405bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Group by season and calculate mean temperature\n",
    "seasonal_temp = df.groupby('Season')['Tmean'].mean()\n",
    "print(\"Average temperature by season:\")\n",
    "print(seasonal_temp.round(2))\n",
    "\n",
    "# Percentage of rainy months by month name\n",
    "rainy_pct = df.groupby('Month_Name')['Rainy_Month'].mean() * 100\n",
    "print(\"\\nRainiest months:\")\n",
    "print(rainy_pct.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51218b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exporting Cleaned Data\n",
    "\n",
    "After preprocessing, **save your cleaned data** for future use. Pandas supports **CSV** (`.to_csv()`), **Excel** (`.to_excel()`), **JSON**, **SQL**, and more. Some formats require additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404f037",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns to export\n",
    "df_export = df[['Year', 'Month', 'Tmax', 'Tmin', 'Rain', 'Sun']]\n",
    "\n",
    "# Export to CSV\n",
    "df_export.to_csv('weather_cleaned.csv')\n",
    "print(\"Exported to CSV\")\n",
    "\n",
    "# Export to Excel (requires openpyxl)\n",
    "# df_export.to_excel('weather_cleaned.xlsx')\n",
    "\n",
    "# Export to JSON\n",
    "# df_export.to_json('weather_cleaned.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739c4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hands on!\n",
    "\n",
    "- You will find a wroked through example in the **lecture** notebook\n",
    "- The exercises will take you through two exercises using real world datasets to practice the concepts covered  today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d62b7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}